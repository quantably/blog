{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/empirical-ai/","result":{"data":{"site":{"siteMetadata":{"title":"Peter Wooldridge","siteUrl":"https://quantably.co","author":{"name":"Peter Wooldridge"}}},"markdownRemark":{"id":"9c46dd23-a7b9-51f4-9e07-98b7d80e3d88","excerpt":"A friend is helping run an AI course. Lately, she's been inundated with what essentially boils down to the same question: Which LLM model should I use?","html":"<p>A friend is helping run an AI course. Lately, she's been inundated with what essentially boils down to the same question:</p>\n<blockquote>\n<p>Which LLM model should I use?</p>\n</blockquote>\n<!-- excerpt -->\n<p>Different people, different projects but the same hunt for certainty.</p>\n<p>Her answer never changes:</p>\n<blockquote>\n<p>You need to run an experiment because you don't know and I don't know.</p>\n</blockquote>\n<p>It’s not the answer they want - but it’s the only one that works.</p>\n<p>It’s a pattern I’ve seen often, especially with those newer to AI development. People want definitive answers but AI doesn't work like that.</p>\n<p>This post explores how to shift from seeking certainty to building an experimental mindset - and how that shift leads to better decisions, faster progress, and ultimately better outcomes.</p>\n<h3>The Empirical Reality</h3>\n<p>If you don't know which model to use, the good news is: nobody else does either. It's not a lack of expertise. It's the nature of AI.</p>\n<p>Context shapes everything. Your data, your constraints, your requirements - they create a performance landscape unique to you.</p>\n<p>Generic model benchmarks exist but these tend to offer signals, not guarantees.\nSome patterns are clear: certain models handle images better than text. Some excel only with structured data. But beyond basic heuristics, the hierarchy breaks down.</p>\n<p>That's why experienced practitioners default to experimentation. They don't assume optimal solutions exist - they experiment. Every project becomes a new question that needs validation.</p>\n<p>School rewards knowing the \"right\" answer. AI requires asking better questions.</p>\n<h4>Why Empiricism Wins</h4>\n<p>In AI, not knowing isn't a weakness. It's your edge - if you know how to use it.</p>\n<p>The most effective AI teams don't hide uncertainty - they operationalise it. They adopt not-knowing into their workflow and cultivate it through disciplined experimentation.</p>\n<p>The most dangerous phrase in AI development? \"Looks good to me.\"</p>\n<p>Moving from opinion based decisions to evidence based ones requires deliberate practice. Here's how to build those habits systematically.</p>\n<h3>Practical Strategies for an Experimental Mindset</h3>\n<p>Building an experimental mindset is like building any habit - start small, and make it deliberate.</p>\n<ul>\n<li>Define metrics that matter for your goals. Technical indicators should correlate with business value:\n<ul>\n<li>For example say you're building a customer service chat bot and the business goal is to reduce support ticket volume. Then the response semantic similiarity score is not the right technical indicator. Something like % of chats that escalate to human agents correlates much closer to the overarching business metric.</li>\n</ul>\n</li>\n<li>Define what \"good enough\" means for your use case before testing anything.</li>\n<li>Build a basic, no-frills solution first. This gives you a testable anchor for comparing more complex approaches.</li>\n<li>Plan your evaluation methodology deliberately. What data will you use? How will you acquire such data? This deserves dedicated planning time.</li>\n<li>Use real data where possible, or synthetic samples generated from real user queries.</li>\n<li>Design experiments to isolate what you're actually testing. Random experimentation teaches nothing.</li>\n<li>Track why things didn't work with the same attention you give to what did. Failed experiments are compressed learning.</li>\n<li>Debate with metrics, not opinions. Show the data or acknowledge it's just preference.</li>\n</ul>\n<h3>Conclusion</h3>\n<p>Certainty is not a viable strategy in AI. The best practitioners don't chase answers - they build better experiments.</p>\n<p>The job of AI leaders is to foster this mindset and create the psychological safety that lets teams fail forward. This means creating space for \"I don't know\" during project planning. It means celebrating experiments that fail fast over assumptions that fail slowly.</p>\n<p>My friend's answer to those questions remains the same: \"You need to run an experiment because you don't know and I don't know.\"</p>\n<p>It's still not the answer people want. But teams that embrace it move faster and make better decisions because of it.</p>","fields":{"folder":"empirical-ai"},"frontmatter":{"title":"The AI Empiricist: Why Experimentation Trumps Authority","date":"June 11, 2025","description":null,"tags":["AI","Business Strategy","Projects"],"summary":null,"headerImage":null}},"previous":{"fields":{"slug":"/stop-scaling-everything/"},"frontmatter":{"title":"Stop Scaling Everything"}},"next":null},"pageContext":{"id":"9c46dd23-a7b9-51f4-9e07-98b7d80e3d88","previousPostId":"24665f6e-ea8f-59e9-91b6-14c8ac4c9b52","nextPostId":null}},"staticQueryHashes":["2841359383"],"slicesMap":{}}