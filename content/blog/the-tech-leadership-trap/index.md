---
title: "Beyond Technical Excellence: The Reality of Leading AI Teams"
date: 2025-04-09
draft: true
---

When I first moved from an IC role into managing AI teams, I worried about the standard stuff: How do I run a good 1:1? How do I delegate effectively? Turns out, while those things matter, they weren't the real challenges that kept me up at night. The unique aspects of leading in the AI sphere demanded a different kind of learning curve.

Honestly, I've sat through several leadership trainings, and I'd struggle to recall much from them. My real education in AI leadership has been forged in the fires of trial and error – figuring things out, making mistakes, and observing firsthand what actually moves the needle. This post isn't about textbook theory; it's a collection of hard-won principles that guide how I operate today. As they say, success doesn't teach much, so consider these lessons learned primarily from the mistakes I've made along the way.

### 1. Focus on Outcomes, Not Just Inputs

As an IC, my focus was on inputs:

- "I completed this Jira ticket"
- "I added this regularisation to this model"
- "I created a new synthetic dataset for benchmarking"

Working with seasoned tech leaders revealed a different approach: they focused on **outcomes**. Their weekly "Progress/Problems/Plan" emails highlighted results, even unsuccessful ones:

>"Had 3 final round interviews for head of X, none successful, 2 more lined up next week"

Thinking in outcomes offers several benefits:

- It aligns with what the business and customers actually care about - this helps guard against busywork disconnected from the actual goal.
- It helps to stay centered and see the results in a packed schedule. 

I found one of the most effective ways to remain outcome oriented is by keeping a weekly log of outcomes as work gets completed. This became invaluable for tracking where my time went and focusing my efforts for the week ahead.

### 2. Speak Everyone's Language (Not Just Tech)

Interviewing senior candidates highlighted a critical skill: tailoring the conversation to the audience. As ICs, we often default to technical jargon, which can alienate other groups. An AI leader must quickly switch between high-level abstraction and low-level details, acting as a bridge between engineers and executives like the CTO.

My top tips for communicating technical topics more broadly:

- **Be concise:** Less is usually more.
- **Focus on business impact:** Instead of "we improved recall by 10%," say "our new system captures 10% more known cases."
- **Ask better questions:** A lesson stolen from Tim Ferriss, this dramatically improved my communication.

Bridging the gap between functional groups requires clear, concise, tailored communication. Understanding how to convey the same message effectively to a Data Scientist, Sales Leader, or Product Manager is invaluable. LLMs can be great tools for practicing this reframing.

This is even more crucial in the field of AI where there is a lot of jargon. Practicing translating technical topics into business ones, is a great exercise. Instead of:

> "High recall but low precision"

say

> "Our system finds almost all the anomalies, but generates some false alarms that we need to filter."

### 3. Drowning in Meetings? Own Your Calendar

As teams and companies scale, it's easy to get overwhelmed by initiatives and meetings. At its peak I've found myself doing 25+ hours of synchronous meetings in a week. My perspective shifted dramatically when someone told me:

>"It's your calendar; you are in charge of it. You can choose what you go to or not."

This simple statement made me question if I was just going through the motions. Were too many meetings a sign I hadn't delegated enough? It forced me to deeply consider my priorities and where my involvement was most impactful. I started encouraging others to take my place in certain meetings and report back. It sounds obvious, but we often assume we need to cover all the bases. Sometimes, dropping the ego and realising others are capable isn't just okay—it empowers them to level up.

### 4. Alignment > Agreement

As a new manager, I often sought consensus. A particularly bad example of this was when trying to agree coding standards for the team. Everyone had different opinions, and consensus was ultimately impossible.
Consensus is rare and often unnecessary. What *is* necessary:

- **People need to feel heard:** Truly heard, not just listened to.
- **The group needs alignment:** The ability to "disagree and commit."

Aiming for universal agreement is futile as teams grow. As a leader, your job is to capture signals and distill them into a decision the team can align around. Achieving alignment is incredibly difficult if people don't feel heard first, often leading to poor outcomes later.

### 5. Trade-offs, Not Roadblocks

Tech leaders spend significant time creating roadmaps. In early-stage companies, these often don't extend beyond a 3-6 month timeline. Unexpected work can and will inevitably come to you. My initial reaction was often resistance - calling the new work a 'bad idea' or 'not strategic.'

This approach, however clear I thought I was, led to being ignored, and initiatives proceeded without my buy-in. 

I realised there was a better way.

First, I shifted from resistance to curiosity. What was the requestor trying to achieve? How could I understand their perspective? Assuming positive intent and seeking to understand made the other person feel heard and improved my own grasp of the situation.

Second, recognise the three options for unplanned work:

1. Absorb it into the current plan.
2. Drop or adjust planned work to accommodate the new request.
3. Don't do the new work.

Options 1 (assuming slack exists) and 3 (potentially shutting down important ideas) are usually poor starting points. I learned to focus on option 2: presenting clear trade-offs.

Often, a simple table works best:

| Impact of New Initiative | Effect on Workstream 1                       | Effect on Workstream 2 | Effect on Workstream 3 | 
|--------------------------|----------------------------------------------|--------------------------|--------------------------|
| Preparing demo       | Can still ship, but Feature X delayed to Q+1 | ...                      | Unaffected               |

This converts a "should we/shouldn't we" debate into a relative prioritisation discussion. Stakeholders feel involved, understand the consequences, and see that you've thought through the impacts. This drastically improved cross-functional alignment and clarity around unplanned work.

### 6. Own the Outcome, Not Every Task

>"Can you take care of our AI company IP?"

"Sure." Cue me spending weeks curating technical documents, gathering UI justifications, and liaising with attorneys. I used to equate ownership with end-to-end delivery.

There's a difference. Ownership means taking accountability for the final outcome (e.g., a patent, a published research paper). It doesn't mean doing all the work yourself. As a leader, you own the outcome and have the ability and responsibility—to enlist help.

The nuance here is figuring out when you are best placed to do the work itself and when delegating is not the right option. That is a judgement call that you hone over time.

### 7. Let Go to Level Up

One of my hardest challenges for me was letting go, especially of coding. After a decade+ of coding, it felt like an identity crisis to stop doing this. If I wasn't building stuff, what was I doing?

Trying to stay deep in the technical details while managing is a recipe for burnout. The crucial reframe was to instead focus on what I'm *gaining*: broader business perspectives to guide the team, closer collaboration with product to connect algorithms to our UI. Letting go of old responsibilities allows you to step into the new, higher-leverage ones demanded by leadership.

### 8. Communicating ML Uncertainy

Estimating technical work is hard, and applied AI research is even harder because you never know when the next breakthrough will land. But research still needs guidance - that's a key job for a good leader. While you might not be able to predict the final outcome with certainty, a leader must clearly articulate the hypotheses being tested and what the team has learned, even from experiments that didn't pan out. The leader's job is to help the team navigate the uncertain world of AI development without completely abandoning the need for a plan.

One of the best ways I've found to explain how an AI team's deliverables differ from a standard engineering team's is to focus on *learning* as the desired outcome. It can be just as valuable to have 'failures' that tell you what *not* to do, helping to refine understanding and shape the next steps.
